{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8beb036f",
      "metadata": {
        "id": "8beb036f"
      },
      "source": [
        "# CS 5588 — Week 2 Hands-On: Applied RAG for Product & Venture Development (Two-Step)\n",
        "**Initiation (20 min, Jan 27)** → **Completion (60 min, Jan 29)**\n",
        "\n",
        "**Submission:** Survey + GitHub  \n",
        "**Due:** **Jan 29 (Thu), end of class**\n",
        "\n",
        "## New Requirement (Important)\n",
        "For **full credit (2% individual)** you must:\n",
        "1) Use **your own project-aligned dataset** (not only benchmark)  \n",
        "2) Add **your own explanations** for key steps\n",
        "\n",
        "### ✅ “Cell Description” rule (same style as CS 5542)\n",
        "After each **IMPORTANT** code cell, add a short Markdown **Cell Description** (2–5 sentences):\n",
        "- What the cell does\n",
        "- Why it matters for a **product-grade** RAG system\n",
        "- Any design choices (chunk size, α, reranker, etc.)\n",
        "\n",
        "> Treat these descriptions as **mini system documentation** (engineering + product thinking).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e43e2d",
      "metadata": {
        "id": "d0e43e2d"
      },
      "source": [
        "## Project Dataset Guide (Required for Full Credit)\n",
        "\n",
        "### Minimum requirements\n",
        "- **5–25 documents** (start small; scale later)\n",
        "- Prefer **plain text** documents (`.txt`)\n",
        "- Put files in a folder named: `project_data/`\n",
        "\n",
        "### Recommended dataset types (choose one)\n",
        "- Policies / guidelines / compliance docs\n",
        "- Technical docs / manuals / SOPs\n",
        "- Customer support FAQs / tickets (de-identified)\n",
        "- Research notes / literature summaries\n",
        "- Domain corpus (healthcare, cybersecurity, business, etc.)\n",
        "\n",
        "> Benchmarks are optional, but **cannot** earn full credit by themselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f68d33",
      "metadata": {
        "id": "e7f68d33"
      },
      "source": [
        "## 0) One-Click Setup + Import Check  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "If you are in **Google Colab**, run the install cell below, then **Runtime → Restart session** if imports fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddaa1c18",
      "metadata": {
        "id": "ddaa1c18",
        "outputId": "10977f46-8a80-46d6-b428-48d2de68afba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mPython: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "✅ If imports fail later: Runtime → Restart session and run again.\n"
          ]
        }
      ],
      "source": [
        "# CS 5588 Lab 2 — One-click dependency install (Colab)\n",
        "!pip -q install -U sentence-transformers chromadb faiss-cpu scikit-learn rank-bm25 transformers accelerate\n",
        "\n",
        "import sys, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"✅ If imports fail later: Runtime → Restart session and run again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab532915",
      "metadata": {
        "id": "ab532915"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences explaining what the setup cell does and why restarting the runtime sometimes matters after pip installs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up and installs  everything needed to run the RAG system by installing all required libraries in one step. Doing this upfront avoids version conflicts and makes the notebook easy for others to run without extra setup. Printing the Python and platform info helps quickly spot environment-related issues if something breaks. The restart note is a practical safeguard for Colab quirks so the workflow stays smooth."
      ],
      "metadata": {
        "id": "qGjJHVFZxLhw"
      },
      "id": "qGjJHVFZxLhw"
    },
    {
      "cell_type": "markdown",
      "id": "49154e13",
      "metadata": {
        "id": "49154e13"
      },
      "source": [
        "# STEP 1 — INITIATION (Jan 27, 20 minutes)\n",
        "**Goal:** Define the **product**, **users**, **dataset reality**, and **trust risks**.\n",
        "\n",
        "> This is a **product milestone**, not a coding demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58216603",
      "metadata": {
        "id": "58216603"
      },
      "source": [
        "## 1A) Product Framing (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Fill in the template below like a founder/product lead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214ee1ba",
      "metadata": {
        "id": "214ee1ba",
        "outputId": "b562ef56-5700-4fd5-dc74-4e4295717cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product_name': 'AI-Powered Weather & Climate Intelligence System for Personalized Decision Support',\n",
              " 'target_users': 'General users, travelers, researchers, and analysts who need actionable weather insights for travel, daily activities, or climate research.',\n",
              " 'core_problem': 'Existing apps mainly focus on forecasts and alertsbut lack contextual interpretation, personalization and explainable insights',\n",
              " 'why_rag_not_chatbot': 'RAG enables the system to retrieve real-time weather data, historical climate records and severe weather alerts that are accurate and graunded. A standard bot is relies on pre-trained knowledge and could hallucinate or provide outdated weather information.',\n",
              " 'failure_harms_who_and_how': 'Incorrect forecasts, misinterpreted AI advice, or missed severe weather alerts could mislead users, causing travel disruptions, safety risks, or property damage. Poor explainability could also reduce trust in the system.'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "product = {\n",
        "  \"product_name\": \"AI-Powered Weather & Climate Intelligence System for Personalized Decision Support\",\n",
        "  \"target_users\": \"General users, travelers, researchers, and analysts who need actionable weather insights for travel, daily activities, or climate research.\",\n",
        "  \"core_problem\": \"Existing apps mainly focus on forecasts and alertsbut lack contextual interpretation, personalization and explainable insights\",\n",
        "  \"why_rag_not_chatbot\": \"RAG enables the system to retrieve real-time weather data, historical climate records and severe weather alerts that are accurate and graunded. A standard bot is relies on pre-trained knowledge and could hallucinate or provide outdated weather information.\",\n",
        "  \"failure_harms_who_and_how\": \"Incorrect forecasts, misinterpreted AI advice, or missed severe weather alerts could mislead users, causing travel disruptions, safety risks, or property damage. Poor explainability could also reduce trust in the system.\",\n",
        "}\n",
        "product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490a084a",
      "metadata": {
        "id": "490a084a"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain your product in 3–5 sentences: who the user is, what pain point exists today, and why grounded RAG helps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This AI-powered weather system helps users—travelers, students, or anyone planning daily activities—get clear and useful weather information. Most apps only show forecasts and alerts, leaving users to figure out what it means. Our system combines real-time weather, historical trends, and severe weather alerts to give personalized, easy-to-understand guidance. Using RAG, the AI bases its answers on real data, so users get accurate and trustworthy recommendations. This helps people plan trips, daily activities, and stay safe during extreme weather."
      ],
      "metadata": {
        "id": "-nO4oJnAoHgA"
      },
      "id": "-nO4oJnAoHgA"
    },
    {
      "cell_type": "markdown",
      "id": "179e8e12",
      "metadata": {
        "id": "179e8e12"
      },
      "source": [
        "## 1B) Dataset Reality Plan (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Describe where your data comes from **in the real world**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282cb6f9",
      "metadata": {
        "id": "282cb6f9",
        "outputId": "860362b0-80bd-40cd-9221-4b295d65e617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_owner': 'Public agencies and APIs (NOAA, NASA, city open data portals, OpenWeather API)',\n",
              " 'data_sensitivity': 'Public, no confidential data involved',\n",
              " 'document_types': 'Real-time weather readings, historical climate data, severe weather alerts, sensor and IoT feeds, geospatial data',\n",
              " 'expected_scale_in_production': 'Millions of data points daily from multiple cities; historical datasets spanning years for trend analysis',\n",
              " 'data_reality_check_paragraph': 'The system gets real-time weather data from APIs like OpenWeather and city open data portals, while historical climate data comes from NOAA and NASA. The data comes in different formats—like JSON, CSV, or NetCDF—so it needs to be cleaned and standardized before use. Real-time APIs can have rate limits or occasional downtime, and historical datasets may have missing or inconsistent records. By handling these challenges carefully, the system can provide accurate forecasts, trend analysis, and personalized weather insights to users.”'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset_plan = {\n",
        "  \"data_owner\": \"Public agencies and APIs (NOAA, NASA, city open data portals, OpenWeather API)\",              # company / agency / public / internal team\n",
        "  \"data_sensitivity\": \"Public, no confidential data involved\",        # public / internal / regulated / confidential\n",
        "  \"document_types\": \"Real-time weather readings, historical climate data, severe weather alerts, sensor and IoT feeds, geospatial data\",          # policies, manuals, reports, research, etc.\n",
        "  \"expected_scale_in_production\": \"Millions of data points daily from multiple cities; historical datasets spanning years for trend analysis\",  # e.g., 200 docs, 10k docs, etc.\n",
        "  \"data_reality_check_paragraph\": \"The system gets real-time weather data from APIs like OpenWeather and city open data portals, while historical climate data comes from NOAA and NASA. The data comes in different formats—like JSON, CSV, or NetCDF—so it needs to be cleaned and standardized before use. Real-time APIs can have rate limits or occasional downtime, and historical datasets may have missing or inconsistent records. By handling these challenges carefully, the system can provide accurate forecasts, trend analysis, and personalized weather insights to users.”\",\n",
        "}\n",
        "dataset_plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2da001",
      "metadata": {
        "id": "3e2da001"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences describing where this data would come from in a real deployment and any privacy/regulatory constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a real deployment, our system would collect real-time weather from services like OpenWeather and city open data portals, while historical climate records come from NOAA and NASA. The data comes in different formats and sometimes has missing or inconsistent entries, so we clean and standardize it before use. All the sources are public, so privacy isn’t an issue, but we still need to handle large volumes carefully to make sure users always get accurate and reliable weather insights."
      ],
      "metadata": {
        "id": "eoH1rEX_pW7z"
      },
      "id": "eoH1rEX_pW7z"
    },
    {
      "cell_type": "markdown",
      "id": "2df3ac72",
      "metadata": {
        "id": "2df3ac72"
      },
      "source": [
        "## 1C) User Stories + Mini Rubric (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Define **3 user stories** (U1 normal, U2 high-stakes, U3 ambiguous/failure) + rubric for evidence and correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a72b8eb",
      "metadata": {
        "id": "0a72b8eb",
        "outputId": "d6f3422b-3f4b-45f0-b594-220f24d2280c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U1_normal': {'user_story': 'As a general user, I want to check the current weather and hourly forecast for my city so that I can plan my day effectively.',\n",
              "  'acceptable_evidence': ['The dashboard shows real-time weather and hourly forecasts for the selected city.',\n",
              "   'User can see temperature, precipitation, wind, and humidity for their location.'],\n",
              "  'correct_answer_must_include': ['Real-time weather data is displayed accurately',\n",
              "   'Forecast information is easily readable and understandable']},\n",
              " 'U2_high_stakes': {'user_story': 'As a traveler, I want to receive alerts for severe weather events so that I can I can avoid dangerous conditions and plan my trips safely..',\n",
              "  'acceptable_evidence': ['System sends notifications or shows alerts for floods, storms, snow, or hurricanes.',\n",
              "   'User can view details of the alert and recommended precautions.'],\n",
              "  'correct_answer_must_include': ['Severe weather alerts are timely and accurate',\n",
              "   'AI provides actionable advice based on the alerts']},\n",
              " 'U3_ambiguous_failure': {'user_story': 'As a researcher, I want to compare current weather with historical trends so that I can understand anomalies or patterns over time',\n",
              "  'acceptable_evidence': ['Dashboard visualizes historical climate trends alongside current data.',\n",
              "   'User can analyze deviations from typical patterns for their city or region.'],\n",
              "  'correct_answer_must_include': ['Historical climate data is correctly integrated and visualized',\n",
              "   'Anomalies or patterns are highlighted clearly']}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "user_stories = {\n",
        "  \"U1_normal\": {\n",
        "    \"user_story\": \"As a general user, I want to check the current weather and hourly forecast for my city so that I can plan my day effectively.\",\n",
        "    \"acceptable_evidence\": [\"The dashboard shows real-time weather and hourly forecasts for the selected city.\",\n",
        "        \"User can see temperature, precipitation, wind, and humidity for their location.\"],\n",
        "    \"correct_answer_must_include\": [\"Real-time weather data is displayed accurately\", \"Forecast information is easily readable and understandable\"],\n",
        "  },\n",
        "  \"U2_high_stakes\": {\n",
        "    \"user_story\": \"As a traveler, I want to receive alerts for severe weather events so that I can I can avoid dangerous conditions and plan my trips safely..\",\n",
        "    \"acceptable_evidence\":[ \"System sends notifications or shows alerts for floods, storms, snow, or hurricanes.\",\n",
        "        \"User can view details of the alert and recommended precautions.\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\"Severe weather alerts are timely and accurate\",\n",
        "        \"AI provides actionable advice based on the alerts\"],\n",
        "  },\n",
        "  \"U3_ambiguous_failure\": {\n",
        "    \"user_story\": \"As a researcher, I want to compare current weather with historical trends so that I can understand anomalies or patterns over time\",\n",
        "    \"acceptable_evidence\": [\"Dashboard visualizes historical climate trends alongside current data.\",\n",
        "        \"User can analyze deviations from typical patterns for their city or region.\"],\n",
        "    \"correct_answer_must_include\": [\"Historical climate data is correctly integrated and visualized\",\n",
        "        \"Anomalies or patterns are highlighted clearly\"],\n",
        "  },\n",
        "}\n",
        "user_stories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5189f5",
      "metadata": {
        "id": "8d5189f5"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why U2 is “high-stakes” and what the system must do to avoid harm (abstain, cite evidence, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "U2 is considered “high-stakes” because it affects users safety—if the system misses a severe weather alert, users could be caught in dangerous situations like floods, storms, or snowstorms. To prevent harm, the system must use trusted sources like NOAA, NASA, and city open data portals, show clearly where the information comes from, and explain the alert in simple terms. If the data is uncertain or missing, the system should warn users and avoid giving misleading advice, so they can make safe, informed decisions.\n"
      ],
      "metadata": {
        "id": "QrV7HyFMrptG"
      },
      "id": "QrV7HyFMrptG"
    },
    {
      "cell_type": "markdown",
      "id": "3b9c075c",
      "metadata": {
        "id": "3b9c075c"
      },
      "source": [
        "## 1D) Trust & Risk Table (Required)\n",
        "Fill at least **3 rows**. These risks should match your product and user stories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972f5b88",
      "metadata": {
        "id": "972f5b88",
        "outputId": "34e606f2-4af9-453a-ca65-3bdc4dcea9c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'risk': 'Hallucination',\n",
              "  'example_failure': 'The AI says there will be a snowstorm in New York tomorrow, but no such storm is predicted in any data source.',\n",
              "  'real_world_consequence': 'Users might cancel plans unnecessarily or panic, reducing trust in the system.',\n",
              "  'safeguard_idea': 'Force citations from trusted sources (NOAA, NASA, city portals) and abstain if data is insufficient.'},\n",
              " {'risk': 'Omission',\n",
              "  'example_failure': 'The system fails to alert users about an approaching hurricane because the data wasn’t retrieved properly.',\n",
              "  'real_world_consequence': 'Users could be caught in dangerous conditions, risking life, health, and property.',\n",
              "  'safeguard_idea': 'Use recall tuning and hybrid retrieval from multiple sources to ensure alerts are not missed.'},\n",
              " {'risk': 'Bias/Misleading',\n",
              "  'example_failure': 'The AI highlights favorable weather for one city while underestimating nearby regions at risk of severe weather.',\n",
              "  'real_world_consequence': 'Users may make unsafe travel or activity decisions, lowering trust and increasing exposure to hazards.',\n",
              "  'safeguard_idea': 'Apply reranking rules, incorporate human review for critical alerts, and clearly communicate uncertainty.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "risk_table = [\n",
        "  {\n",
        "    \"risk\": \"Hallucination\",\n",
        "    \"example_failure\": \"The AI says there will be a snowstorm in New York tomorrow, but no such storm is predicted in any data source.\",\n",
        "    \"real_world_consequence\": \"Users might cancel plans unnecessarily or panic, reducing trust in the system.\",\n",
        "    \"safeguard_idea\": \"Force citations from trusted sources (NOAA, NASA, city portals) and abstain if data is insufficient.\"\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Omission\",\n",
        "    \"example_failure\": \"The system fails to alert users about an approaching hurricane because the data wasn’t retrieved properly.\",\n",
        "    \"real_world_consequence\": \"Users could be caught in dangerous conditions, risking life, health, and property.\",\n",
        "    \"safeguard_idea\": \"Use recall tuning and hybrid retrieval from multiple sources to ensure alerts are not missed.\"\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Bias/Misleading\",\n",
        "    \"example_failure\": \"The AI highlights favorable weather for one city while underestimating nearby regions at risk of severe weather.\",\n",
        "    \"real_world_consequence\": \"Users may make unsafe travel or activity decisions, lowering trust and increasing exposure to hazards.\",\n",
        "    \"safeguard_idea\": \"Apply reranking rules, incorporate human review for critical alerts, and clearly communicate uncertainty.\"\n",
        "  },\n",
        "]\n",
        "\n",
        "risk_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe422b",
      "metadata": {
        "id": "33fe422b"
      },
      "source": [
        "✅ **Step 1 Checkpoint (End of Jan 27)**\n",
        "Commit (or submit) your filled templates:\n",
        "- `product`, `dataset_plan`, `user_stories`, `risk_table`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9645a53",
      "metadata": {
        "id": "b9645a53"
      },
      "source": [
        "# STEP 2 — COMPLETION (Jan 29, 60 minutes)\n",
        "**Goal:** Build a working **product-grade** RAG pipeline:\n",
        "Chunking → Keyword + Vector Retrieval → Hybrid α → Governance Rerank → Grounded Answer → Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849ea98a",
      "metadata": {
        "id": "849ea98a"
      },
      "source": [
        "## 2A) Project Dataset Setup (Required for Full Credit)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "\n",
        "### Colab Upload Tips\n",
        "- Left sidebar → **Files** → Upload `.txt`\n",
        "- Place them into `project_data/`\n",
        "\n",
        "This cell creates the folder and shows how many files were found.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a38f48",
      "metadata": {
        "id": "90a38f48",
        "outputId": "9fcad6ec-5a84-4e0a-f49d-afbe605516eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ project_data/ ready | moved: 0 | files: 0\n",
            "Example files: []\n"
          ]
        }
      ],
      "source": [
        "import os, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "# (Optional helper) Move any .txt in current directory into project_data/\n",
        "moved = 0\n",
        "for fp in glob.glob(\"*.txt\"):\n",
        "    shutil.move(fp, os.path.join(PROJECT_FOLDER, os.path.basename(fp)))\n",
        "    moved += 1\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\")))\n",
        "print(\"✅ project_data/ ready | moved:\", moved, \"| files:\", len(files))\n",
        "print(\"Example files:\", files[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec380ad4",
      "metadata": {
        "id": "ec380ad4"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "List what dataset you used, how many docs, and why they reflect your product scenario (not just a toy example).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a487a1c7",
      "metadata": {
        "id": "a487a1c7"
      },
      "source": [
        "## 2B) Load Documents + Build Chunks  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This milestone cell loads `.txt` documents and produces chunks using either **fixed** or **semantic** chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a081d6",
      "metadata": {
        "id": "13a081d6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def load_project_docs(folder=\"project_data\", max_docs=25):\n",
        "    paths = sorted(Path(folder).glob(\"*.txt\"))[:max_docs]\n",
        "    docs = []\n",
        "    for p in paths:\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "        if txt:\n",
        "            docs.append({\"doc_id\": p.name, \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "def fixed_chunk(text, chunk_size=900, overlap=150):\n",
        "    # Character-based chunking for speed + simplicity\n",
        "    chunks, i = [], 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "        i += (chunk_size - overlap)\n",
        "    return [c.strip() for c in chunks if c.strip()]\n",
        "\n",
        "def semantic_chunk(text, max_chars=1000):\n",
        "    # Paragraph-based packing\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 2 <= max_chars:\n",
        "            cur = (cur + \"\\n\\n\" + p).strip()\n",
        "        else:\n",
        "            if cur: chunks.append(cur)\n",
        "            cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "# ---- Choose chunking policy ----\n",
        "CHUNKING = \"semantic\"   # \"fixed\" or \"semantic\"\n",
        "FIXED_SIZE = 900\n",
        "FIXED_OVERLAP = 150\n",
        "SEM_MAX = 1000\n",
        "\n",
        "docs = load_project_docs(PROJECT_FOLDER, max_docs=25)\n",
        "print(\"Loaded docs:\", len(docs))\n",
        "\n",
        "all_chunks = []\n",
        "for d in docs:\n",
        "    chunks = fixed_chunk(d[\"text\"], FIXED_SIZE, FIXED_OVERLAP) if CHUNKING == \"fixed\" else semantic_chunk(d[\"text\"], SEM_MAX)\n",
        "    for j, c in enumerate(chunks):\n",
        "        all_chunks.append({\"chunk_id\": f'{d[\"doc_id\"]}::c{j}', \"doc_id\": d[\"doc_id\"], \"text\": c})\n",
        "\n",
        "print(\"Chunking:\", CHUNKING, \"| total chunks:\", len(all_chunks))\n",
        "print(\"Sample chunk id:\", all_chunks[0][\"chunk_id\"] if all_chunks else \"NO CHUNKS (upload .txt files first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204e5e83",
      "metadata": {
        "id": "204e5e83"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why you chose fixed vs semantic chunking for your product, and how chunking affects precision/recall and trust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bec9a30",
      "metadata": {
        "id": "9bec9a30"
      },
      "source": [
        "## 2C) Build Retrieval Engines (BM25 + Vector Index)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell builds:\n",
        "- **Keyword retrieval** (BM25) for exact matches / compliance\n",
        "- **Vector retrieval** (embeddings + FAISS) for semantic matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0484f1a",
      "metadata": {
        "id": "d0484f1a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ----- Keyword (BM25) -----\n",
        "tokenized = [c[\"text\"].lower().split() for c in all_chunks]\n",
        "bm25 = BM25Okapi(tokenized) if len(tokenized) else None\n",
        "\n",
        "def keyword_search(query, k=10):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(query.lower().split())\n",
        "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [(all_chunks[i], float(scores[i])) for i in idx]\n",
        "\n",
        "# ----- Vector (Embeddings + FAISS) -----\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "if len(chunk_texts) > 0:\n",
        "    emb = embedder.encode(chunk_texts, show_progress_bar=True, normalize_embeddings=True)\n",
        "    emb = np.asarray(emb, dtype=\"float32\")\n",
        "\n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    index.add(emb)\n",
        "\n",
        "    def vector_search(query, k=10):\n",
        "        q = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "        scores, idx = index.search(q, k)\n",
        "        out = [(all_chunks[int(i)], float(s)) for s, i in zip(scores[0], idx[0])]\n",
        "        return out\n",
        "    print(\"✅ Vector index built | chunks:\", len(all_chunks), \"| dim:\", emb.shape[1])\n",
        "else:\n",
        "    index = None\n",
        "    def vector_search(query, k=10): return []\n",
        "    print(\"⚠️ No chunks found. Upload .txt files to project_data/ and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cb1a14",
      "metadata": {
        "id": "c7cb1a14"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why your product needs both keyword and vector retrieval (what each catches that the other misses).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7dfd29",
      "metadata": {
        "id": "3d7dfd29"
      },
      "source": [
        "## 2D) Hybrid Retrieval (α Fusion Policy)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Hybrid score = **α · keyword + (1 − α) · vector** after simple normalization.\n",
        "\n",
        "Try α ∈ {0.2, 0.5, 0.8} and justify your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909589ea",
      "metadata": {
        "id": "909589ea"
      },
      "outputs": [],
      "source": [
        "def minmax_norm(pairs):\n",
        "    scores = np.array([s for _, s in pairs], dtype=\"float32\") if pairs else np.array([], dtype=\"float32\")\n",
        "    if len(scores) == 0:\n",
        "        return []\n",
        "    mn, mx = float(scores.min()), float(scores.max())\n",
        "    if mx - mn < 1e-8:\n",
        "        return [(c, 1.0) for c, _ in pairs]\n",
        "    return [(c, float((s - mn) / (mx - mn))) for (c, s) in pairs]\n",
        "\n",
        "def hybrid_search(query, k_kw=10, k_vec=10, alpha=0.5, k_out=10):\n",
        "    kw = keyword_search(query, k_kw)\n",
        "    vc = vector_search(query, k_vec)\n",
        "    kw_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(kw))\n",
        "    vc_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(vc))\n",
        "\n",
        "    ids = set(kw_n) | set(vc_n)\n",
        "    fused = []\n",
        "    for cid in ids:\n",
        "        s = alpha * kw_n.get(cid, 0.0) + (1 - alpha) * vc_n.get(cid, 0.0)\n",
        "        chunk = next(c for c in all_chunks if c[\"chunk_id\"] == cid)\n",
        "        fused.append((chunk, float(s)))\n",
        "\n",
        "    fused.sort(key=lambda x: x[1], reverse=True)\n",
        "    return fused[:k_out]\n",
        "\n",
        "ALPHA = 0.5  # try 0.2 / 0.5 / 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4b3559",
      "metadata": {
        "id": "3a4b3559"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe your user type (precision-first vs discovery-first) and why your α choice fits that user and risk profile.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f888bf",
      "metadata": {
        "id": "b1f888bf"
      },
      "source": [
        "## 2E) Governance Layer (Re-ranking)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Re-ranking is treated as **governance** (risk reduction), not just performance tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e2fb25",
      "metadata": {
        "id": "d8e2fb25"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANK_MODEL) if RERANK else None\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    if reranker is None or len(candidates) == 0:\n",
        "        return candidates\n",
        "    pairs = [(query, c[\"text\"]) for c, _ in candidates]\n",
        "    scores = reranker.predict(pairs)\n",
        "    out = [(c, float(s)) for (c, _), s in zip(candidates, scores)]\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "    return out\n",
        "\n",
        "print(\"✅ Reranker:\", RERANK_MODEL if RERANK else \"OFF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bb530f",
      "metadata": {
        "id": "16bb530f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what “governance” means for your product and what failure this reranking step helps prevent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81bbbd3",
      "metadata": {
        "id": "d81bbbd3"
      },
      "source": [
        "## 2F) Grounded Answer + Citations  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "We include a lightweight generation option, plus a fallback mode.\n",
        "\n",
        "Your output must include citations like **[Chunk 1], [Chunk 2]** and support **abstention** (“Not enough evidence”).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605ae6d1",
      "metadata": {
        "id": "605ae6d1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "USE_LLM = False  # set True to generate; keep False if downloads are slow\n",
        "GEN_MODEL = \"google/flan-t5-base\"\n",
        "\n",
        "gen = pipeline(\"text2text-generation\", model=GEN_MODEL) if USE_LLM else None\n",
        "\n",
        "def build_context(top_chunks, max_chars=2500):\n",
        "    ctx = \"\"\n",
        "    for i, (c, _) in enumerate(top_chunks, start=1):\n",
        "        block = f\"[Chunk {i}] {c['text'].strip()}\\n\"\n",
        "        if len(ctx) + len(block) > max_chars:\n",
        "            break\n",
        "        ctx += block + \"\\n\"\n",
        "    return ctx.strip()\n",
        "\n",
        "def rag_answer(query, top_chunks):\n",
        "    ctx = build_context(top_chunks)\n",
        "    if USE_LLM and gen is not None:\n",
        "        prompt = (\n",
        "            \"Answer the question using ONLY the evidence below. \"\n",
        "            \"If there is not enough evidence, say 'Not enough evidence.' \"\n",
        "            \"Include citations like [Chunk 1], [Chunk 2].\\n\\n\"\n",
        "            f\"Question: {query}\\n\\nEvidence:\\n{ctx}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out = gen(prompt, max_new_tokens=180)[0][\"generated_text\"]\n",
        "        return out, ctx\n",
        "    else:\n",
        "        # fallback: evidence-first placeholder\n",
        "        answer = (\n",
        "            \"Evidence summary (fallback mode):\\n\"\n",
        "            + \"\\n\".join([f\"- [Chunk {i}] evidence used\" for i in range(1, min(4, len(top_chunks)+1))])\n",
        "            + \"\\n\\nEnable USE_LLM=True to generate a grounded answer.\"\n",
        "        )\n",
        "        return answer, ctx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50ed74",
      "metadata": {
        "id": "0c50ed74"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how citations and abstention improve trust in your product, especially for U2 (high-stakes) and U3 (ambiguous).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78586432",
      "metadata": {
        "id": "78586432"
      },
      "source": [
        "## 2G) Run the Pipeline on Your 3 User Stories  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell turns your user stories into concrete queries, runs hybrid+rerank, and prints results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606aaafa",
      "metadata": {
        "id": "606aaafa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def story_to_query(story_text):\n",
        "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
        "    return m.group(1).strip() if m else story_text.strip()\n",
        "\n",
        "queries = [\n",
        "    (\"U1_normal\", story_to_query(user_stories[\"U1_normal\"][\"user_story\"])),\n",
        "    (\"U2_high_stakes\", story_to_query(user_stories[\"U2_high_stakes\"][\"user_story\"])),\n",
        "    (\"U3_ambiguous_failure\", story_to_query(user_stories[\"U3_ambiguous_failure\"][\"user_story\"])),\n",
        "]\n",
        "\n",
        "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
        "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
        "    ranked = rerank(query, base) if do_rerank else base\n",
        "    top5 = ranked[:5]\n",
        "    ans, ctx = rag_answer(query, top5[:3])\n",
        "    return top5, ans, ctx\n",
        "\n",
        "results = {}\n",
        "for key, q in queries:\n",
        "    top5, ans, ctx = run_pipeline(q)\n",
        "    results[key] = {\"query\": q, \"top5\": top5, \"answer\": ans, \"context\": ctx}\n",
        "\n",
        "for key in results:\n",
        "    print(\"\\n===\", key, \"===\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top chunk ids:\", [c[\"chunk_id\"] for c, _ in results[key][\"top5\"][:3]])\n",
        "    print(\"Answer preview:\\n\", results[key][\"answer\"][:500], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ae35f7",
      "metadata": {
        "id": "e1ae35f7"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe one place where the system helped (better grounding) and one place where it struggled (which layer and why).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62b369e",
      "metadata": {
        "id": "b62b369e"
      },
      "source": [
        "## 2H) Evaluation (Technical + Product)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Use your rubric to label relevance and compute Precision@5 / Recall@10.\n",
        "Also assign product scores: Trust (1–5) and Decision Confidence (1–5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7a7869",
      "metadata": {
        "id": "9d7a7869"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(relevant_flags, k=5):\n",
        "    rel = relevant_flags[:k]\n",
        "    return sum(rel) / max(1, len(rel))\n",
        "\n",
        "def recall_at_k(relevant_flags, total_relevant, k=10):\n",
        "    rel_found = sum(relevant_flags[:k])\n",
        "    return rel_found / max(1, total_relevant)\n",
        "\n",
        "evaluation = {}\n",
        "for key in results:\n",
        "    print(\"\\n---\", key, \"---\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top-5 chunks:\")\n",
        "    for i, (c, s) in enumerate(results[key][\"top5\"], start=1):\n",
        "        print(i, c[\"chunk_id\"], \"| score:\", round(s, 3))\n",
        "\n",
        "    evaluation[key] = {\n",
        "        \"relevant_flags_top10\": [0]*10,             # set 1 for each relevant chunk among top-10\n",
        "        \"total_relevant_chunks_estimate\": 0,        # estimate from your rubric\n",
        "        \"precision_at_5\": None,\n",
        "        \"recall_at_10\": None,\n",
        "        \"trust_score_1to5\": 0,\n",
        "        \"confidence_score_1to5\": 0,\n",
        "    }\n",
        "\n",
        "evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1991f",
      "metadata": {
        "id": "92f1991f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how you labeled “relevance” using your rubric and what “trust” means for your target users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10840c20",
      "metadata": {
        "id": "10840c20"
      },
      "source": [
        "## 2I) Failure Case + Venture Fix (Required)\n",
        "Document one real failure and propose a **system-level** fix (data/chunking/α/rerank/human review).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717d394e",
      "metadata": {
        "id": "717d394e"
      },
      "outputs": [],
      "source": [
        "failure_case = {\n",
        "  \"which_user_story\": \"\",\n",
        "  \"what_failed\": \"\",\n",
        "  \"which_layer_failed\": \"\",  # Chunking / Retrieval / Re-ranking / Generation\n",
        "  \"real_world_consequence\": \"\",\n",
        "  \"proposed_system_fix\": \"\",\n",
        "}\n",
        "failure_case\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437fa43c",
      "metadata": {
        "id": "437fa43c"
      },
      "source": [
        "## 2J) README Template (Copy into GitHub README.md)\n",
        "\n",
        "```md\n",
        "# Week 2 Hands-On — Applied RAG Product Results (CS 5588)\n",
        "\n",
        "## Product Overview\n",
        "- Product name:\n",
        "- Target users:\n",
        "- Core problem:\n",
        "- Why RAG:\n",
        "\n",
        "## Dataset Reality\n",
        "- Source / owner:\n",
        "- Sensitivity:\n",
        "- Document types:\n",
        "- Expected scale in production:\n",
        "\n",
        "## User Stories + Rubric\n",
        "- U1:\n",
        "- U2:\n",
        "- U3:\n",
        "(Rubric: acceptable evidence + correct answer criteria)\n",
        "\n",
        "## System Architecture\n",
        "- Chunking:\n",
        "- Keyword retrieval:\n",
        "- Vector retrieval:\n",
        "- Hybrid α:\n",
        "- Reranking governance:\n",
        "- LLM / generation option:\n",
        "\n",
        "## Results\n",
        "| User Story | Method | Precision@5 | Recall@10 | Trust (1–5) | Confidence (1–5) |\n",
        "|---|---|---:|---:|---:|---:|\n",
        "\n",
        "## Failure + Fix\n",
        "- Failure:\n",
        "- Layer:\n",
        "- Consequence:\n",
        "- Safeguard / next fix:\n",
        "\n",
        "## Evidence of Grounding\n",
        "Paste one RAG answer with citations: [Chunk 1], [Chunk 2]\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}